{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      " [[1 2]\n",
      " [3 4]] \n",
      " [[1, 2], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data = [[1, 2], [3, 4]]\n",
    "\n",
    "np_array = np.array(data) # Create Numpy object by list\n",
    "t= torch.tensor(data) # Create tensor object by list\n",
    "tensor_from_np = torch.from_numpy(np_array) # Create tensor object by numpy\n",
    "\n",
    "np_from_tensor = t.numpy() # Transform Tensor to numpy\n",
    "listdata = tensor_from_np.tolist() # Transform Tensor to list\n",
    "\n",
    "print(t, '\\n', np_from_tensor, '\\n', listdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从另一个张量对象复制数据来初始化新的张量\n",
    "\n",
    "tensor_data = t.clone() # 返回和t相同的tensor， 存在新的内存中\n",
    "new_data = t.detach() # 返回与t相同的tensor，新对象和旧对象共用内存\n",
    "\n",
    "ones_data = torch.ones_like(t) # 和t形状一致的全1张量\n",
    "zeros_data = torch.zeros_like(t) # 和t形状一致的全0张量\n",
    "rand_data = torch.rand_like(t, dtype=torch.float) # 和t形状一致的随机浮点数张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: torch.Size([3, 4])\n",
      "tensor type: torch.Size([3, 4])\n",
      "tensor device: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"tensor shape: {tensor.shape}\")\n",
    "print(f\"tensor type: {tensor.shape}\")\n",
    "print(f\"tensor device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Can cuda boost?\n",
    "print(device)\n",
    "\n",
    "tensor = tensor.to(device) # put tensor to GPU\n",
    "print(f\"tensor device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice & Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st line: tensor([44, 79, 47,  8])\n",
      "1st column: tensor([44, 84,  3, 83])\n",
      "last column: tensor([ 8, 90, 79,  4])\n",
      "tensor([[44,  0, 47,  8],\n",
      "        [84,  0, 31, 90],\n",
      "        [ 3,  0, 49, 79],\n",
      "        [83,  0, 63,  4]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randint(1, 100, (4,4)) #(low, high, size)\n",
    "\n",
    "print(f\"1st line: {tensor[0]}\")\n",
    "print(f\"1st column: {tensor[:,0]}\")\n",
    "print(f\"last column: {tensor[..., -1]}\") #... is same to :\n",
    "tensor[:, 1] = 0 # replace the second column to 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat\n",
    "\n",
    "除了要连接的维度值不同，其他维度的值应保持一致，否则不可以进行连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([3, 4]) torch.Size([4, 2]) torch.Size([7, 4]) torch.Size([4, 6])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.randint(1,100,(4, 4))\n",
    "tensor2 = torch.rand(3,4) \n",
    "tensor3 = torch.rand(4,2) \n",
    "\n",
    "t1 = torch.cat([tensor1, tensor2], dim=0) # 纵向连接\n",
    "t2 = torch.cat([tensor1, tensor3], dim=1) # 横向连接 ()[]都行\n",
    "print(tensor1.shape, tensor2.shape, tensor3.shape, t1.shape, t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([3, 2, 4]) torch.Size([2, 3, 4]) torch.Size([2, 4, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[47.,  4., 45.,  3.],\n",
       "          [59., 53., 95., 77.]],\n",
       " \n",
       "         [[ 0.,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 1.,  1.,  1.,  1.],\n",
       "          [ 1.,  1.,  1.,  1.]]]),\n",
       " tensor([[[47.,  4., 45.,  3.],\n",
       "          [ 0.,  0.,  0.,  0.],\n",
       "          [ 1.,  1.,  1.,  1.]],\n",
       " \n",
       "         [[59., 53., 95., 77.],\n",
       "          [ 0.,  0.,  0.,  0.],\n",
       "          [ 1.,  1.,  1.,  1.]]]),\n",
       " tensor([[[47.,  0.,  1.],\n",
       "          [ 4.,  0.,  1.],\n",
       "          [45.,  0.,  1.],\n",
       "          [ 3.,  0.,  1.]],\n",
       " \n",
       "         [[59.,  0.,  1.],\n",
       "          [53.,  0.,  1.],\n",
       "          [95.,  0.,  1.],\n",
       "          [77.,  0.,  1.]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stack 各个维度都应该一致\n",
    "\n",
    "tensor1 = torch.randint(1,100,(2,4))\n",
    "tensor2 = torch.zeros(2,4)\n",
    "tensor3 = torch.ones(2,4)\n",
    "\n",
    "t1 = torch.stack([tensor1, tensor2, tensor3],dim=0)\n",
    "t2 = torch.stack([tensor1, tensor2, tensor3],dim=1)\n",
    "t3 = torch.stack([tensor1, tensor2, tensor3],dim=2)\n",
    "print(tensor1.shape, tensor2.shape, tensor3.shape, t1.shape, t2.shape, t3.shape)\n",
    "t1,t2,t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.randint(1,100,(2,4))\n",
    "tensor2 = torch.ones(2,4)\n",
    "\n",
    "# + 以下123是一样的\n",
    "t_add1 = tensor1 + tensor2\n",
    "t_add2 = tensor1.add(tensor2)\n",
    "t_add3 = torch.add(tensor1, tensor2)\n",
    "\n",
    "# 456是一样的，张量所有元素都加3，得到新的张量，原张量未改变\n",
    "t_add4 = tensor1 +3\n",
    "t_add5 = tensor1.add(3)\n",
    "t_add6 = torch.add(tensor1, 3)\n",
    "\n",
    "t_add7 = tensor1.add_(3) # 张量所有元素都加3，原张量tensor1也被修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -\n",
    "t_sub1 = tensor1 - tensor2\n",
    "t_sub2 = tensor1.sub(tensor2)\n",
    "t_sub3 = torch.sub(tensor1, tensor2)\n",
    "\n",
    "# 张量乘法\n",
    "t_matmul1 = tensor.float() @ tensor2.T # 单精度相乘\n",
    "t_matmul2 = tensor1.matmul(tensor2.T.long()) #long相乘\n",
    "\n",
    "# 按元素相乘\n",
    "t_mul1 = tensor1 * 3\n",
    "t_mul2 = tensor.mul(3)\n",
    "t_mul3 = torch.mul(tensor1, 3)\n",
    "\n",
    "# /\n",
    "t_div1 = tensor1 / tensor2\n",
    "t_div2 = tensor1.div(tensor2)\n",
    "t_div3 = torch.div(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特殊运算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "torch.Size([2])\n",
      "tensor([[True, True]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn((5)) # 1dim tensor\n",
    "t2 = torch.ones((5)) # 1dim tensor\n",
    "t3 = torch.randn((2,5)) # 2dim tensor\n",
    "t4 = torch.ones((5,2)) # 2dim tensor\n",
    "\n",
    "t_d1 = torch.dot(t1, t2) # 1dim 内积\n",
    "t_d2 = torch.matmul(t1, t2) # same to dot\n",
    "print(t_d1 == t_d2)\n",
    "\n",
    "t_m1 = torch.mm(t3, t4) ## mm support 2dim mul\n",
    "t_m2 = torch.matmul(t3, t4)\n",
    "print(t_m1 == t_m2)\n",
    "\n",
    "t_n1 = torch.matmul(t3, t1) # dim1 * dim2 dim1会扩展维度，结果会删掉扩展的维度\n",
    "print(t_n1.shape)\n",
    "\n",
    "t_n2 = torch.matmul(t3, t1.view(5,1)).T # 手动扩展进行计算\n",
    "print(t_n1 == t_n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 4, 7, 8, 5, 3, 2, 6, 0, 1])\n",
      "tensor(45) <class 'torch.Tensor'>\n",
      "45 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# 取张量的某个元素进行普通数据类型的运算 用item()\n",
    "\n",
    "tensor = torch.randperm(10)\n",
    "sum = tensor.sum()\n",
    "sum_item = sum.item()\n",
    "\n",
    "print(tensor)\n",
    "print(sum, type(sum))\n",
    "print(sum_item, type(sum_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "设x和y是真实数据，z为模型预测值，设z = w * x + b，则w和b是模型需要优化的参数，通过loss损失可根据梯度下降法来优化w和b。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2385, 0.2310, 0.2653],\n",
      "        [0.2385, 0.2310, 0.2653],\n",
      "        [0.2385, 0.2310, 0.2653],\n",
      "        [0.2385, 0.2310, 0.2653],\n",
      "        [0.2385, 0.2310, 0.2653]])\n",
      "tensor([0.2385, 0.2310, 0.2653])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True) # (shape=5*3, 梯度计算True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x,w) + b\n",
    "\n",
    "loss = binary_cross_entropy_with_logits(z, y) # 计算损失根据loss来优化网络参数\n",
    "loss.backward() # 损失反向传播进行自动求导，得到参数梯度\n",
    "\n",
    "print(w.grad) # 输出w的梯度，存储在w的grad属性中，grad属性也是张量\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后就可以利用学习率进行修正\n",
    "\n",
    "默认情况下，所有张量的requires_grad属性被设置为True，表示都在跟踪梯度历史，以下是禁用跟踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "# 禁用跟踪\n",
    "with torch.no_grad():\n",
    "    z = z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "\n",
    "# 也可以用这个\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
