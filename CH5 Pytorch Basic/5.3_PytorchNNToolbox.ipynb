{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要介绍nn模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.1 nn.Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 33])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "conv1 = nn.Conv1d(in_channels=256, out_channels=100, kernel_size=3 ,stride=1, padding=0) # define a 1 dim conv\n",
    "input = torch.randn(32, 35, 256) # define input feature tensor (batch_size, MaxLength, feature dimension)\n",
    "input = input.permute(0, 2, 1) # switch the dimension of tensor to (batch_size, feature dimension, MaxLength) 开始运算前需要将L换到最后一个维度让kernel进行卷积\n",
    "out = conv1(input) #进行一维卷积操作，输出特征图张量形状为[batch_size, out_channels, (L + 2 * padding - kernel_size) / stride + 1]\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[ 0.0079, -0.0332,  0.0343],\n",
      "         [-0.0286, -0.0011, -0.0111],\n",
      "         [ 0.0156,  0.0100,  0.0314],\n",
      "         ...,\n",
      "         [ 0.0304, -0.0355,  0.0035],\n",
      "         [-0.0308,  0.0218,  0.0012],\n",
      "         [-0.0290, -0.0116, -0.0175]],\n",
      "\n",
      "        [[ 0.0227, -0.0354, -0.0066],\n",
      "         [ 0.0249,  0.0147, -0.0351],\n",
      "         [-0.0139, -0.0212,  0.0295],\n",
      "         ...,\n",
      "         [ 0.0016,  0.0317,  0.0038],\n",
      "         [-0.0248, -0.0359, -0.0059],\n",
      "         [-0.0251,  0.0065, -0.0181]],\n",
      "\n",
      "        [[-0.0088,  0.0324, -0.0150],\n",
      "         [-0.0157,  0.0344,  0.0255],\n",
      "         [ 0.0317,  0.0332, -0.0192],\n",
      "         ...,\n",
      "         [ 0.0311,  0.0292,  0.0238],\n",
      "         [-0.0346, -0.0097, -0.0293],\n",
      "         [ 0.0227,  0.0093, -0.0051]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0266,  0.0141, -0.0359],\n",
      "         [-0.0345,  0.0206, -0.0151],\n",
      "         [-0.0309, -0.0004, -0.0119],\n",
      "         ...,\n",
      "         [-0.0185,  0.0020, -0.0111],\n",
      "         [-0.0341,  0.0107,  0.0256],\n",
      "         [ 0.0263,  0.0335,  0.0272]],\n",
      "\n",
      "        [[-0.0008, -0.0361,  0.0222],\n",
      "         [-0.0073,  0.0130,  0.0035],\n",
      "         [-0.0009, -0.0106,  0.0109],\n",
      "         ...,\n",
      "         [-0.0165, -0.0125,  0.0107],\n",
      "         [-0.0284,  0.0115, -0.0118],\n",
      "         [ 0.0187, -0.0226, -0.0223]],\n",
      "\n",
      "        [[ 0.0108, -0.0286, -0.0133],\n",
      "         [-0.0128, -0.0230,  0.0026],\n",
      "         [ 0.0333, -0.0169, -0.0132],\n",
      "         ...,\n",
      "         [ 0.0264, -0.0161,  0.0291],\n",
      "         [ 0.0014, -0.0292,  0.0059],\n",
      "         [-0.0261,  0.0083,  0.0045]]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0014,  0.0178,  0.0262,  0.0051, -0.0340,  0.0102, -0.0159,  0.0197,\n",
      "         0.0200, -0.0275, -0.0180, -0.0126,  0.0269,  0.0293,  0.0180,  0.0106,\n",
      "        -0.0304, -0.0330, -0.0033,  0.0274,  0.0193, -0.0129, -0.0127, -0.0299,\n",
      "         0.0097, -0.0212,  0.0019, -0.0267,  0.0221,  0.0216,  0.0237,  0.0348,\n",
      "         0.0205,  0.0186,  0.0269, -0.0270,  0.0228,  0.0038,  0.0244,  0.0010,\n",
      "        -0.0254,  0.0118, -0.0213, -0.0184,  0.0073,  0.0298,  0.0294, -0.0079,\n",
      "        -0.0109,  0.0057, -0.0331,  0.0131,  0.0202, -0.0240, -0.0251,  0.0077,\n",
      "         0.0225, -0.0333,  0.0164, -0.0222, -0.0206,  0.0320,  0.0043,  0.0059,\n",
      "         0.0323,  0.0165,  0.0103,  0.0214,  0.0277,  0.0186, -0.0231, -0.0058,\n",
      "         0.0295, -0.0068, -0.0075,  0.0057,  0.0244,  0.0093,  0.0034, -0.0254,\n",
      "         0.0005, -0.0014, -0.0323,  0.0157, -0.0224,  0.0186, -0.0100,  0.0043,\n",
      "         0.0244,  0.0123,  0.0016,  0.0269,  0.0324, -0.0283, -0.0254,  0.0316,\n",
      "        -0.0204, -0.0088,  0.0130,  0.0318], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 打印所有参数当前值\n",
    "print(list(conv1.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Conv2d\n",
    "\n",
    "卷积核从左到右，从上到下对图像进行卷积操作\n",
    "\n",
    "二维卷积对宽和高的卷积结果形状都为[batch_size, out_channels, (L + 2 * padding - kernel_size) / stride + 1]，其中L为矩阵数据的宽或高，stride都为1，padding默认为0，所以输出张量横向和纵向的维度均为(64+2*0-3)/1 + 1 = 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 62, 62])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 16, 64, 64) # (batch_size, channel, height, width)\n",
    "m = nn.Conv2d(16, 32, (3, 3), (1, 1)) # in_channel, out_channel, kernel_size, stride\n",
    "y = m(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[[ 0.0312,  0.0599,  0.0217],\n",
      "          [ 0.0624,  0.0422,  0.0084],\n",
      "          [ 0.0078,  0.0817, -0.0359]],\n",
      "\n",
      "         [[-0.0771, -0.0167,  0.0064],\n",
      "          [-0.0540,  0.0435,  0.0106],\n",
      "          [ 0.0257, -0.0201,  0.0173]],\n",
      "\n",
      "         [[ 0.0419,  0.0138, -0.0548],\n",
      "          [ 0.0191,  0.0438,  0.0200],\n",
      "          [ 0.0299, -0.0613, -0.0237]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0149, -0.0584,  0.0164],\n",
      "          [-0.0688,  0.0410,  0.0627],\n",
      "          [-0.0152,  0.0444,  0.0588]],\n",
      "\n",
      "         [[-0.0189,  0.0058, -0.0275],\n",
      "          [-0.0404,  0.0725, -0.0242],\n",
      "          [ 0.0479, -0.0612, -0.0368]],\n",
      "\n",
      "         [[ 0.0452, -0.0783,  0.0655],\n",
      "          [ 0.0196,  0.0742,  0.0597],\n",
      "          [ 0.0483,  0.0034, -0.0283]]],\n",
      "\n",
      "\n",
      "        [[[-0.0522, -0.0470,  0.0036],\n",
      "          [ 0.0775,  0.0184,  0.0620],\n",
      "          [ 0.0809,  0.0075,  0.0564]],\n",
      "\n",
      "         [[ 0.0207, -0.0596,  0.0609],\n",
      "          [ 0.0455,  0.0793, -0.0639],\n",
      "          [ 0.0288, -0.0567, -0.0696]],\n",
      "\n",
      "         [[ 0.0382,  0.0022,  0.0154],\n",
      "          [ 0.0655,  0.0674, -0.0823],\n",
      "          [-0.0312,  0.0798,  0.0536]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0372,  0.0196, -0.0579],\n",
      "          [-0.0693, -0.0642, -0.0275],\n",
      "          [-0.0573,  0.0280, -0.0437]],\n",
      "\n",
      "         [[ 0.0570,  0.0605, -0.0149],\n",
      "          [-0.0832, -0.0594, -0.0633],\n",
      "          [-0.0150, -0.0349, -0.0109]],\n",
      "\n",
      "         [[ 0.0100, -0.0593,  0.0649],\n",
      "          [ 0.0145,  0.0735, -0.0800],\n",
      "          [-0.0224,  0.0785,  0.0295]]],\n",
      "\n",
      "\n",
      "        [[[-0.0110,  0.0040,  0.0424],\n",
      "          [ 0.0055, -0.0331,  0.0106],\n",
      "          [-0.0627,  0.0567,  0.0009]],\n",
      "\n",
      "         [[ 0.0475, -0.0818, -0.0738],\n",
      "          [ 0.0141,  0.0017, -0.0470],\n",
      "          [ 0.0614, -0.0092, -0.0582]],\n",
      "\n",
      "         [[ 0.0257, -0.0324, -0.0645],\n",
      "          [ 0.0556, -0.0832,  0.0360],\n",
      "          [ 0.0242, -0.0276, -0.0178]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0709,  0.0530,  0.0687],\n",
      "          [ 0.0300,  0.0137, -0.0410],\n",
      "          [-0.0589,  0.0649,  0.0071]],\n",
      "\n",
      "         [[ 0.0052, -0.0795,  0.0009],\n",
      "          [-0.0058, -0.0341, -0.0414],\n",
      "          [ 0.0543,  0.0306,  0.0739]],\n",
      "\n",
      "         [[ 0.0602,  0.0470,  0.0373],\n",
      "          [-0.0767,  0.0526, -0.0135],\n",
      "          [ 0.0218, -0.0133,  0.0197]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0094,  0.0101, -0.0134],\n",
      "          [-0.0794, -0.0470,  0.0550],\n",
      "          [-0.0449, -0.0634, -0.0620]],\n",
      "\n",
      "         [[ 0.0325, -0.0767,  0.0402],\n",
      "          [ 0.0796,  0.0199,  0.0236],\n",
      "          [ 0.0673, -0.0597, -0.0172]],\n",
      "\n",
      "         [[ 0.0513,  0.0514,  0.0651],\n",
      "          [-0.0213, -0.0232, -0.0131],\n",
      "          [ 0.0515, -0.0344, -0.0688]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0062, -0.0424, -0.0802],\n",
      "          [-0.0233, -0.0050, -0.0129],\n",
      "          [ 0.0808,  0.0758, -0.0047]],\n",
      "\n",
      "         [[-0.0287, -0.0281, -0.0056],\n",
      "          [ 0.0575, -0.0607,  0.0269],\n",
      "          [-0.0817,  0.0605, -0.0667]],\n",
      "\n",
      "         [[-0.0625, -0.0091,  0.0036],\n",
      "          [ 0.0776, -0.0476,  0.0249],\n",
      "          [-0.0265,  0.0764,  0.0028]]],\n",
      "\n",
      "\n",
      "        [[[-0.0249, -0.0391, -0.0021],\n",
      "          [-0.0787, -0.0429, -0.0236],\n",
      "          [-0.0276,  0.0773,  0.0348]],\n",
      "\n",
      "         [[ 0.0762,  0.0288,  0.0191],\n",
      "          [-0.0212, -0.0528, -0.0344],\n",
      "          [ 0.0667, -0.0617,  0.0515]],\n",
      "\n",
      "         [[ 0.0365,  0.0552,  0.0146],\n",
      "          [-0.0246, -0.0447, -0.0658],\n",
      "          [ 0.0139, -0.0295, -0.0118]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0570,  0.0083, -0.0592],\n",
      "          [-0.0391, -0.0314,  0.0060],\n",
      "          [ 0.0031, -0.0433,  0.0143]],\n",
      "\n",
      "         [[ 0.0511, -0.0709, -0.0522],\n",
      "          [-0.0441,  0.0369,  0.0206],\n",
      "          [ 0.0359, -0.0302, -0.0621]],\n",
      "\n",
      "         [[-0.0395, -0.0408,  0.0130],\n",
      "          [ 0.0549, -0.0310,  0.0577],\n",
      "          [ 0.0745,  0.0173, -0.0690]]],\n",
      "\n",
      "\n",
      "        [[[-0.0425, -0.0426,  0.0019],\n",
      "          [-0.0820,  0.0301,  0.0669],\n",
      "          [-0.0626, -0.0750, -0.0076]],\n",
      "\n",
      "         [[-0.0631, -0.0596,  0.0117],\n",
      "          [ 0.0382, -0.0307, -0.0607],\n",
      "          [-0.0480, -0.0282,  0.0412]],\n",
      "\n",
      "         [[-0.0333,  0.0538,  0.0406],\n",
      "          [ 0.0376, -0.0615, -0.0142],\n",
      "          [-0.0661, -0.0638,  0.0088]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0309, -0.0756, -0.0750],\n",
      "          [-0.0760, -0.0035, -0.0104],\n",
      "          [-0.0697,  0.0127, -0.0166]],\n",
      "\n",
      "         [[-0.0124,  0.0331,  0.0238],\n",
      "          [ 0.0403,  0.0054,  0.0204],\n",
      "          [ 0.0674, -0.0472,  0.0077]],\n",
      "\n",
      "         [[ 0.0178, -0.0569, -0.0649],\n",
      "          [-0.0150, -0.0293, -0.0546],\n",
      "          [-0.0534,  0.0378,  0.0749]]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0419, -0.0109, -0.0809,  0.0606,  0.0216, -0.0664,  0.0742,  0.0727,\n",
      "        -0.0239,  0.0797,  0.0694, -0.0763,  0.0393, -0.0640,  0.0481,  0.0203,\n",
      "        -0.0451,  0.0470,  0.0268,  0.0419, -0.0171, -0.0670, -0.0817, -0.0318,\n",
      "        -0.0284, -0.0571,  0.0088,  0.0828,  0.0157,  0.0151,  0.0338,  0.0137],\n",
      "       requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(m.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全连接 nn.Linear\n",
    "\n",
    "nn.Linear(in_features, out_features, bias=True) \n",
    "\n",
    "参数in_features表示输入维度的大小，out_features表示输出维度的大小，bias表示是否带偏置，默认是带偏置的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "connected_layer = nn.Linear(in_features = 64*64*3, out_features = 1)\n",
    "input = torch.randn(10, 3, 64, 64)\n",
    "input = input.view(10, 64*64*3) # 用于改变张量的形状，也可以用input.reshape((10, 64*64*3))\n",
    "output = connected_layer(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0048, -0.0056,  0.0074,  ..., -0.0005,  0.0024, -0.0065]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0084], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(connected_layer.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平坦化类 nn.Flatten\n",
    "\n",
    "input = input.view(10, 64* 64*3)语句实现了对张量形状的调整。实际上，nn模块还提供了Flatten类，也可以直接把指定的连续几维数据展平为连续的一维数据，默认从第1维到最后一维进行平坦化，第0维常表示batch_size，因此不进行展平。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面的view()也可以写成\n",
    "\n",
    "Flatten = nn.Flatten() #实例化\n",
    "input = Flatten(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Flatten作为一种操作，可以放到顺序化容器（nn.Sequential）中，更具有通用性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非线性激活函数\n",
    "\n",
    "用来对模型的输入和输出构建复杂的映射。例如ReLU、Softmax、Sigmoid、Tanh、LogSigmoid、LogSoftmax等。激活函数常用于在线性变换后，通过加入非线性变换使得模型能进行更复杂的表示。以nn.ReLU为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ReLU(inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "Before ReLU: tensor([[ 0.0543,  0.8503, -0.2232,  0.6797,  0.7454],\n",
      "        [-0.5675,  0.5837,  0.0578, -0.2704,  0.0245],\n",
      "        [ 0.4450,  1.0845,  0.7304, -0.3484,  0.3928],\n",
      "        [-0.2471, -0.0282, -0.1664, -0.2699,  0.0721]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0543, 0.8503, 0.0000, 0.6797, 0.7454],\n",
      "        [0.0000, 0.5837, 0.0578, 0.0000, 0.0245],\n",
      "        [0.4450, 1.0845, 0.7304, 0.0000, 0.3928],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0721]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 示例：线性层引入非线性层\n",
    "input = torch.randn(4, 3, 64, 64)\n",
    "Flatten = nn.Flatten() # Instantiate\n",
    "flat_image = Flatten(input)\n",
    "layer1 = nn.Linear(in_features=64*64*3, out_features=5)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见负数都变成0了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 顺序化容器 nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （1）定义时直接加入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 0.1262,  0.1395],\n",
      "        [-0.1480, -0.0952],\n",
      "        [ 0.0540, -0.2990],\n",
      "        [ 0.5258,  0.2561]], grad_fn=<AddmmBackward0>)\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  (3): Linear(in_features=123008, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(4, 3, 64, 64)\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, (3,3), (1,1)),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32*62*62, 2),\n",
    ")\n",
    "output = net(input)\n",
    "print(f\"output: {output}\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （2）先定义对象，后加入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_module方法\n",
    "net = nn.Sequential()\n",
    "net.add_module('conv1', nn.Conv2d(16, 32, (3,3), (1,1)))\n",
    "net.add_module('relu', nn.ReLU())\n",
    "net.add_module('flatten', nn.Flatten())\n",
    "net.add_module('linear', nn.Linear(32*62*62, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （3）定义时传入有序字典作为参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "net = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(16, 32, (3,3), (1,1))),\n",
    "    ('relu', nn.ReLU()),\n",
    "    ('flattern', nn.Flatten()),\n",
    "    ('linear', nn.Linear(32*62*62, 1)),\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该方法将有序字典作为参数传入，各个神经网络模块作为有序字典的元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2593)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "loss = nn.L1Loss()\n",
    "predict_value = torch.randn(1, 23)\n",
    "target = torch.randn(1, 23)\n",
    "output = loss(predict_value, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: tensor([[ 1.5410, -0.2934, -2.1788,  0.5684, -1.0845],\n",
      "        [-1.3986,  0.4033,  0.8380, -0.7193, -0.4033],\n",
      "        [-0.5966,  0.1820, -0.8567,  1.1006, -1.0712]], requires_grad=True)\n",
      "y:tensor([3, 0, 0])\n",
      "loss: 2.272947072982788\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "p = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5) # 得到每个样本实际类别标签\n",
    "print(f\"p: {p}\\ny:{target}\")\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "output = loss(p, target) # loss\n",
    "print(f\"loss: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
